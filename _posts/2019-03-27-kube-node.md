---
layout: post
title: "4.k8s部署-Node组件安装"
date: 2019-03-27 00:55:58
image: '/assets/img/'
description: 'k8s部署第四步-Node组件安装'
tags:
- kubernetes
- node
categories:
- kubernetes
twitter_text: 'k8s部署第四步-Node组件安装'
---

# 4. k8s部署-Node组件

#### [Part 1: kubelet-bootstrap](#part1)

#### [Part 2: 创建kubeconfig文件](#part2)

#### [Part 3: 部署kubelet组件](#part3)

#### [Part 4: 学习排错](#part4)

#### [Part 5: 部署kube-proxy组件](#part5)

#### [Part 6: 扩容Node节点](#part6)

#### [Part 7: 部署一个Nginx测试示例](#part7)

#### [kubernetes二进制部署系列](#serial)

![3.png](https://cdn.nlark.com/yuque/0/2019/png/288708/1553578734176-ec9a5652-ee65-4b00-95a4-f13c27684c2f.png#align=left&display=inline&height=608&name=3.png&originHeight=643&originWidth=789&size=31031&status=done&width=746)

### <a name="part1"></a>1. 将kubelet-bootstrap用户绑定到系统集群角色

为我们之前生成token的用户名设定角色权限

```bash
[k8s@k8s-master01 ~]$ cat /opt/kubernetes/cfg/token.csv 
3172e91e77cb1a5fe47a3c957e94f5f1,kubelet-bootstrap,10001,"system:kubelet-bootstrap"
```

为kubelet-bootstrap用户赋予集群的node-bootstrapper权限

这样 kubernetes 访问集群中的这个 token id

```bash
sudo kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole=system:node-bootstrapper \
--user=kubelet-bootstrap
```

命令行返回  `clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created` 就代表创建成功

### <a name="part2"></a>2. 创建kubeconfig文件

部署 **Node** 组件之前需要先创建 **kubeconfig** 文件 `vi kubeconfig.sh`

```bash
# 创建 TLS Bootstrapping Token
#BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
#BOOTSTRAP_TOKEN=0fb61c46f8991b718eb38d27b605b008

#cat > token.csv <<EOF
#${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
#EOF

#----------------------

APISERVER=$1
SSL_DIR=$2

# 创建kubelet bootstrapping kubeconfig 
export KUBE_APISERVER="https://$APISERVER:6443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=$SSL_DIR/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

#----------------------

# 创建kube-proxy kubeconfig文件

kubectl config set-cluster kubernetes \
  --certificate-authority=$SSL_DIR/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=$SSL_DIR/kube-proxy.pem \
  --client-key=$SSL_DIR/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

我们将之前的 创建 TLS Bootstrapping Token 的代码删除

目的是存放连接 apiserver 的认证信息

保存并执行 `sudo bash kubeconfig.sh` 第一个参数是 apiserver 的地址，第二个参数是证书目录

`bash kubeconfig.sh 10.211.55.10 ~/k8s/k8s-cert`

```bash
[k8s@k8s-master01 k8s-cert]$ ls
admin.csr             ca-config.json  k8s-cert.sh          kube-proxy.kubeconfig  server.pem
admin-csr.json        ca.csr          kubeconfig.sh        kube-proxy.pem         token.csv
admin-key.pem         ca-csr.json     kube-proxy.csr       server.csr
admin.pem             ca-key.pem      kube-proxy-csr.json  server-csr.json
bootstrap.kubeconfig  ca.pem          kube-proxy-key.pem   server-key.pem
[k8s@k8s-master01 k8s-cert]$ sudo bash kubeconfig.sh 10.211.55.10 ~/k8s/k8s-cert
Cluster "kubernetes" set.
User "kubelet-bootstrap" set.
Context "default" modified.
Switched to context "default".
Cluster "kubernetes" set.
User "kube-proxy" set.
Context "default" modified.
Switched to context "default".
[k8s@k8s-master01 k8s-cert]$ ls
admin.csr             ca-config.json  k8s-cert.sh          kube-proxy.kubeconfig  server.pem
admin-csr.json        ca.csr          kubeconfig.sh        kube-proxy.pem         token.csv
admin-key.pem         ca-csr.json     kube-proxy.csr       server.csr
admin.pem             ca-key.pem      kube-proxy-csr.json  server-csr.json
bootstrap.kubeconfig  ca.pem          kube-proxy-key.pem   server-key.pem
```

可以看到上面生成了两个 .**kubeconfig** 文件，让我们来查看下代码 `cat bootstrap.kubeconfig` 

```bash
[k8s@k8s-master01 k8s-cert]$ sudo cat bootstrap.kubeconfig
[sudo] password for k8s: 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVSG5uV0pCdmpFbXF1OUxjaVFrMVpkYy9td1BZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU1TURNeU5ERTRNREF3TUZvWERUSTBNRE15TWpFNE1EQXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbGFXcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBM1pmVTZiTHRzVVFKRnlodnNvbC8KaUhpSUd6dk9URDdXZ0twMnlKTXJzNUEyRVUwZEZSYUNJR0QvS3htT3V2RUhUbHJVOEZvY1BiYzdRUVFqZjh2cgpGbjBPTURtd1RuZmozcUpGSFNTUmN0TURhWjl1eFRpMy9qa2w1VFRDTWZtcXBCc2l3N2ZnOVpmOXpkVVo0Q3psCjF2TU9Odi8zVUZoZk5SQlhqR1hKaTFxakdIZnpuQ0lqVzMxZ1llK2hsYm0ySHlJeU5rUktmWTg4bjFHWG81YXAKWUtFdkJPa1pqVGlSQVNEendhRUMzM1UrNldCcStsTjdveGsybzBmVVlQak9VSm5TZU1ZdEQ4K1V6S1lhVnF4dwpwSWRna0RORXkxOGVQME1nR1drdEI5VW8wYVc2OWpXOW9GRlBvREZUdVFTR01aZ2RiUW0yTUYrT2pCU01wM0NIClNRSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVU2pqODg5TE13cmhOWUVZcWlucWg5dWJSUjhBd0h3WURWUjBqQkJnd0ZvQVVTamo4ODlMTQp3cmhOWUVZcWlucWg5dWJSUjhBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFKL1ltZmFOWU51NUI1T3ROV1BRCmUzU29uMXd2SVJOVHVkNlZ3ZmJTNHdnUXFwMmNVOFB0Unp1Vk5xMDNBZXpOZWFRSjJZZlFBaFdldTdjb2F6cDMKcVY2VG54RE04TWlmZngzQ05CQUliRVRoZE04STliVjJMc3RXNFcxQlR3K1BQNzFwenYyVUs4R0xTUjNMd0RYZQpLM2JpVnBJMkEydmZUVXh2WlFEYVFYa1lpSmdrcjNoVFVnd3FZQ0dNV0tMOFZ3ODJEUk5uNmdiZmRSVW4rWUkyCkFrUXZoYkhualVlYUVxYUhIbVZ1QWZJV2tRbXZNOHcwaW84dEdobkJpcFBNc245bExicEMvbUhiVkFhSE05QzAKTTFpTVN6MWNwQzNoOGdBYzhUNTlSSjJRbmtjVTM0KzdSd2dvWjg4M0FDWHRVVWs0OG51RHEzNzNuc0thZFBNUwpMTTg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://10.211.55.10:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet-bootstrap
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user: {}
[k8s@k8s-master01 k8s-cert]$ 
```

`apiVersion`：apiserver 版本

`clusters`：指定kubernetes集群的ca证书 

`server`：连接apiserver的地址

`contexts`：上下文决定使用哪一个集群，通过名称区分多个集群

`current-context`：默认访问的集群 

因为我们分步骤执行的脚本文件，所以这里缺少了token，我们来手动添加上

```bash
[k8s@k8s-master01 ~]$ cat /opt/kubernetes/cfg/token.csv 
3172e91e77cb1a5fe47a3c957e94f5f1,kubelet-bootstrap,10001,"system:kubelet-bootstrap"
```

这样我们来修改下 `vi kubeconfig.sh` 脚本，再重新生成 .**kubeconfig **文件

```bash
# 创建 TLS Bootstrapping Token
#BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
#BOOTSTRAP_TOKEN=0fb61c46f8991b718eb38d27b605b008

#cat > token.csv <<EOF
#${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
#EOF

#----------------------

APISERVER=$1
SSL_DIR=$2
BOOTSTRAP_TOKEN=3172e91e77cb1a5fe47a3c957e94f5f1

# 创建kubelet bootstrapping kubeconfig 
export KUBE_APISERVER="https://$APISERVER:6443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=$SSL_DIR/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

#----------------------

# 创建kube-proxy kubeconfig文件

kubectl config set-cluster kubernetes \
  --certificate-authority=$SSL_DIR/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=$SSL_DIR/kube-proxy.pem \
  --client-key=$SSL_DIR/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

现在再重新执行一下 `sudo bash kubeconfig.sh 10.211.55.10 ~/k8s/k8s-cert` 

现在再查看一下 `sudo cat bootstrap.kubeconfig`  

```bash
[k8s@k8s-master01 k8s-cert]$ sudo cat bootstrap.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVSG5uV0pCdmpFbXF1OUxjaVFrMVpkYy9td1BZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU1TURNeU5ERTRNREF3TUZvWERUSTBNRE15TWpFNE1EQXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbGFXcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBM1pmVTZiTHRzVVFKRnlodnNvbC8KaUhpSUd6dk9URDdXZ0twMnlKTXJzNUEyRVUwZEZSYUNJR0QvS3htT3V2RUhUbHJVOEZvY1BiYzdRUVFqZjh2cgpGbjBPTURtd1RuZmozcUpGSFNTUmN0TURhWjl1eFRpMy9qa2w1VFRDTWZtcXBCc2l3N2ZnOVpmOXpkVVo0Q3psCjF2TU9Odi8zVUZoZk5SQlhqR1hKaTFxakdIZnpuQ0lqVzMxZ1llK2hsYm0ySHlJeU5rUktmWTg4bjFHWG81YXAKWUtFdkJPa1pqVGlSQVNEendhRUMzM1UrNldCcStsTjdveGsybzBmVVlQak9VSm5TZU1ZdEQ4K1V6S1lhVnF4dwpwSWRna0RORXkxOGVQME1nR1drdEI5VW8wYVc2OWpXOW9GRlBvREZUdVFTR01aZ2RiUW0yTUYrT2pCU01wM0NIClNRSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVU2pqODg5TE13cmhOWUVZcWlucWg5dWJSUjhBd0h3WURWUjBqQkJnd0ZvQVVTamo4ODlMTQp3cmhOWUVZcWlucWg5dWJSUjhBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFKL1ltZmFOWU51NUI1T3ROV1BRCmUzU29uMXd2SVJOVHVkNlZ3ZmJTNHdnUXFwMmNVOFB0Unp1Vk5xMDNBZXpOZWFRSjJZZlFBaFdldTdjb2F6cDMKcVY2VG54RE04TWlmZngzQ05CQUliRVRoZE04STliVjJMc3RXNFcxQlR3K1BQNzFwenYyVUs4R0xTUjNMd0RYZQpLM2JpVnBJMkEydmZUVXh2WlFEYVFYa1lpSmdrcjNoVFVnd3FZQ0dNV0tMOFZ3ODJEUk5uNmdiZmRSVW4rWUkyCkFrUXZoYkhualVlYUVxYUhIbVZ1QWZJV2tRbXZNOHcwaW84dEdobkJpcFBNc245bExicEMvbUhiVkFhSE05QzAKTTFpTVN6MWNwQzNoOGdBYzhUNTlSSjJRbmtjVTM0KzdSd2dvWjg4M0FDWHRVVWs0OG51RHEzNzNuc0thZFBNUwpMTTg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://10.211.55.10:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet-bootstrap
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: 3172e91e77cb1a5fe47a3c957e94f5f1
```

这样 token 就有了，再看下生成的 `sudo cat kube-proxy.kubeconfig` 文件

```bash
[k8s@k8s-master01 k8s-cert]$ sudo cat kube-proxy.kubeconfig 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVSG5uV0pCdmpFbXF1OUxjaVFrMVpkYy9td1BZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU1TURNeU5ERTRNREF3TUZvWERUSTBNRE15TWpFNE1EQXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbGFXcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBM1pmVTZiTHRzVVFKRnlodnNvbC8KaUhpSUd6dk9URDdXZ0twMnlKTXJzNUEyRVUwZEZSYUNJR0QvS3htT3V2RUhUbHJVOEZvY1BiYzdRUVFqZjh2cgpGbjBPTURtd1RuZmozcUpGSFNTUmN0TURhWjl1eFRpMy9qa2w1VFRDTWZtcXBCc2l3N2ZnOVpmOXpkVVo0Q3psCjF2TU9Odi8zVUZoZk5SQlhqR1hKaTFxakdIZnpuQ0lqVzMxZ1llK2hsYm0ySHlJeU5rUktmWTg4bjFHWG81YXAKWUtFdkJPa1pqVGlSQVNEendhRUMzM1UrNldCcStsTjdveGsybzBmVVlQak9VSm5TZU1ZdEQ4K1V6S1lhVnF4dwpwSWRna0RORXkxOGVQME1nR1drdEI5VW8wYVc2OWpXOW9GRlBvREZUdVFTR01aZ2RiUW0yTUYrT2pCU01wM0NIClNRSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVU2pqODg5TE13cmhOWUVZcWlucWg5dWJSUjhBd0h3WURWUjBqQkJnd0ZvQVVTamo4ODlMTQp3cmhOWUVZcWlucWg5dWJSUjhBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFKL1ltZmFOWU51NUI1T3ROV1BRCmUzU29uMXd2SVJOVHVkNlZ3ZmJTNHdnUXFwMmNVOFB0Unp1Vk5xMDNBZXpOZWFRSjJZZlFBaFdldTdjb2F6cDMKcVY2VG54RE04TWlmZngzQ05CQUliRVRoZE04STliVjJMc3RXNFcxQlR3K1BQNzFwenYyVUs4R0xTUjNMd0RYZQpLM2JpVnBJMkEydmZUVXh2WlFEYVFYa1lpSmdrcjNoVFVnd3FZQ0dNV0tMOFZ3ODJEUk5uNmdiZmRSVW4rWUkyCkFrUXZoYkhualVlYUVxYUhIbVZ1QWZJV2tRbXZNOHcwaW84dEdobkJpcFBNc245bExicEMvbUhiVkFhSE05QzAKTTFpTVN6MWNwQzNoOGdBYzhUNTlSSjJRbmtjVTM0KzdSd2dvWjg4M0FDWHRVVWs0OG51RHEzNzNuc0thZFBNUwpMTTg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://10.211.55.10:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-proxy
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kube-proxy
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQzakNDQXNhZ0F3SUJBZ0lVREdqd25ubE10T1VteG1OaVRKcUV5ZkoyRmNVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU1TURNeU5ERTRNREF3TUZvWERUSTVNRE15TVRFNE1EQXdNRm93YkRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbGFVcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUm93R0FZRFZRUURFeEZ6ZVhOMFpXMDZhM1ZpClpTMXdjbTk0ZVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTDFRYVB3TTd2QlIKaVREbG5qM0NiVmhHWGpKVkJvR3lmakJRL3VvaHM2L2p0SEtNdkJuY0htN0JvYzhHM3hoOU1LZkdnczNtamovbgp3WitEL2FObVdIWHdXSmlFYkRRR0UyMmc5dWh4MURvQk8yaE9XTGZGc2xMODdjZVpjUW1rYnViSVNUSWpVTDNjCk9ncTNVQ3gvRnRnc3hxbWdFRGhIemdNdW1uSUFTa3NyQTBPQjdiNFcxczlydzZWc3dSUlZWRmVuMGpXU3NLNlAKaXllNmdrQlNpZEZ0RGFDR2JPYmFYRGRpaWFRbDIwMUVFMEYzVS84U2pxbGYrcXlKd0FOSmM4OVJBakJ3TlFlNQp6RTFnNTVRMndkOUQrVTcxaFVrMG9uMnV0RHUyYzUyVFBvaVBLS1g0LzBJYXJ4eXhMdFpZcldsZHFlTlNJY2lOCjF4WkRMb2YvdmhzQ0F3RUFBYU4vTUgwd0RnWURWUjBQQVFIL0JBUURBZ1dnTUIwR0ExVWRKUVFXTUJRR0NDc0cKQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFNQmdOVkhSTUJBZjhFQWpBQU1CMEdBMVVkRGdRV0JCU0llZ3U3OTdCRQpqbFladW9IWlhtYjNxK2VHbWpBZkJnTlZIU01FR0RBV2dCUktPUHp6MHN6Q3VFMWdSaXFLZXFIMjV0Rkh3REFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQU9ZRGZhZG94TFJRbUsyYks4aWwybGF4MVE2UTJrM3BZL1NtSHlBT0cKWlVyNDErRXB2aXNYQ1Vlak5LdnFudnFodFZvdzVUTUVhSmJGMVB1VW9iWlNQNHM4cFZ4VFBraXB4YzNtRENIWApCSXJ0RjdBVWwvLy9CUkVDTjNINmFPdGJHY0pZRW95UkpzZXdpUUpqMGo0c0gzcUs1R3pSdm5XK3lLRk5VSWdXCk83QzA2U0xvUk5rWWUvVWRhTlF5c0wvZ2svbGxLamMrcUhXektnWVpWUktpUUR3TlRkMGdQZjMyc0lxaEpLaFUKVWZqTUNJSjNMeTBnZ0pEdzhYWitwcEFsaE1XbVhrMEE0MnRmb3hkUkZQcmc4YlZKVVdzYkZHQUFEZWtVZTExaQpaVGZBaXFnbmFiTmZnZHhUWGRpaDY2ZXFqWmhLaG9mV21pV294dUNvRnIvS2tBPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: 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
[k8s@k8s-master01 k8s-cert]$ 
```

### <a name="part3"></a>3. 部署kubelet组件

准备好了 **bootstrap.kubeconfig** 和 **kube-proxy.kubeconfig** 文件之后，先拷贝到 node 节点上

`sudo scp bootstrap.kubeconfig kube-proxy.kubeconfig root@10.211.55.12:/opt/kubernetes/cfg/`

`sudo scp bootstrap.kubeconfig kube-proxy.kubeconfig root@10.211.55.13:/opt/kubernetes/cfg/`

`sudo scp kubelet kube-proxy root@``10.211.55.12:/opt/kubernetes/bin/`<br /><br />`sudo scp kubelet kube-proxy root@``10.211.55.13:/opt/kubernetes/bin/`

等下部署**kubelet**，**kube-proxy **组件 都需要这两个文件，接下来就可以在这两个 **node** 节点上部署 **node** 了

我们现在一个node节点上新建一个`vi kubelet.sh`脚本，该脚本与 apiserver 脚本大体相同

```bash
#!/bin/bash

NODE_ADDRESS=$1
DNS_SERVER_IP=${2:-"10.0.0.2"}

cat <<EOF >/opt/kubernetes/cfg/kubelet

KUBELET_OPTS="--logtostderr=false \\
--log-dir=/opt/kubernetes/logs \\
--v=4 \\
--hostname-override=${NODE_ADDRESS} \\
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\
--config=/opt/kubernetes/cfg/kubelet.config \\
--cert-dir=/opt/kubernetes/ssl \\
--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0"

EOF

cat <<EOF >/opt/kubernetes/cfg/kubelet.config

kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: ${NODE_ADDRESS}
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- ${DNS_SERVER_IP} 
clusterDomain: cluster.local.
failSwapOn: false
authentication:
  anonymous:
    enabled: true
EOF

cat <<EOF >/usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet
ExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
KillMode=process

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable kubelet
systemctl restart kubelet
```

运行脚本，注意需要指定 **NODE_ADDRESS** 地址，**DNS_SERVER_IP **直接使用默认的就可以了

`sudo bash kubelet.sh 10.211.55.12` 

查看下有没有启动 `ps -ef | grep kube`   

查看下生成的配置文件

```bash
[k8s@k8s-node01 node]$ cat /opt/kubernetes/cfg/kubelet

KUBELET_OPTS="--logtostderr=false \
--log-dir=/opt/kubernetes/logs \
--v=4 \
--hostname-override=10.211.55.12 \
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
--config=/opt/kubernetes/cfg/kubelet.config \
--cert-dir=/opt/kubernetes/ssl \
--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0"

[k8s@k8s-node01 node]$ cat /opt/kubernetes/cfg/kubelet.config 

kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 10.211.55.12
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2 
clusterDomain: cluster.local.
failSwapOn: false
authentication:
  anonymous:
    enabled: true
[k8s@k8s-node01 node]$ 
```

这里参数的具体意义：

`logtostderr`：日志目录，记得要有对应的目录，`sudo mkdir /opt/kubernetes/logs`

`--v=4`：日志级别

`--hostname-override`：当前的主机名就是当前的IP

`--kubeconfig`：kubernetes的配置文件

`--bootstrap-kubeconfig`：之前创建的**bootstrap.kubeconfig**文件

`--pod-infra-container-image`：创建Pod 时容器引用的镜像地址，默认的国外服务器可能访问延迟或失败

### <a name="part4"></a>4. 学习排错

现在启动 **kubelet** 是失败的，让我们来一步步排查下问题 `sudo systemctl restart kubelet`

`journalctl -u kubelet` 可以查看失败的原因

`sudo tail /var/log/messages -f` 查看系统日志

`sudo less /var/log/messages -f` 

`less /opt/kubernetes/logs/kubelet.INFO` 查看kubelet生成的日志

`sudo cat /opt/kubernetes/cfg/bootstrap.kubeconfig` 检查配置文件 

`less /opt/kubernetes/logs/kubelet.ERROR` 查看 error 信息

`source /opt/kubenetes/cfg/kubelet` source kubelet配置文件

`/opt/kubernetes/bin/kubelet $KUBELET_OPTS` 然后指定选项进行手动启动

解决之后的显示如下 `sudo systemctl restart kubelet`

```bash
[k8s@k8s-node01 node]$ ps -ef|grep kubelet
root      8737     1  0 20:11 ?        00:00:00 /opt/kubernetes/bin/kubelet --logtostderr=false --log-dir=/opt/kubernetes/logs --v=4 --hostname-override=10.211.55.12 --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig --config=/opt/kubernetes/cfg/kubelet.config --cert-dir=/opt/kubernetes/ssl --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
k8s      11133 15286  0 20:43 pts/3    00:00:00 grep --color=auto kubelet
```

可以在 master 节点上通过 `kubectl get csr` 来获取请求的签名

```bash
[k8s@k8s-master01 bin]$ kubectl get csr
NAME                                                   AGE   REQUESTOR           CONDITION
node-csr-dOkgFXEB9Ee5A060ALeSKQ6Pm4ahYT6XyaQbv_c5lK8   40m   kubelet-bootstrap   Pending
```

通过 `kubectl certificate approve node-csr-**` 来允许颁发

这样就可以通过 `kubectl get node` 查看，这样就长长加入到集群中了

```bash
[k8s@k8s-master01 bin]$ kubectl get csr
NAME                                                   AGE   REQUESTOR           CONDITION
node-csr-dOkgFXEB9Ee5A060ALeSKQ6Pm4ahYT6XyaQbv_c5lK8   40m   kubelet-bootstrap   Pending
[k8s@k8s-master01 bin]$ kubectl get node
No resources found.
[k8s@k8s-master01 bin]$ kubectl certificate approve node-csr-dOkgFXEB9Ee5A060ALeSKQ6Pm4ahYT6XyaQbv_c5lK8
certificatesigningrequest.certificates.k8s.io/node-csr-dOkgFXEB9Ee5A060ALeSKQ6Pm4ahYT6XyaQbv_c5lK8 approved
[k8s@k8s-master01 bin]$ kubectl get node
NAME           STATUS     ROLES    AGE   VERSION
10.211.55.12   NotReady   <none>   9s    v1.13.4
[k8s@k8s-master01 bin]$ kubectl get node
NAME           STATUS   ROLES    AGE   VERSION
10.211.55.12   Ready    <none>   19s   v1.13.4
```

### <a name="part5"></a>5. 部署kube-proxy组件

首先新建 `vi proxy.sh` 脚本

```bash
#!/bin/bash

NODE_ADDRESS=$1

cat <<EOF >/opt/kubernetes/cfg/kube-proxy

KUBE_PROXY_OPTS="--logtostderr=false \\
--log-dir=/opt/kubernetes/logs \\
--v=4 \\
--hostname-override=${NODE_ADDRESS} \\
--cluster-cidr=10.0.0.0/24 \\
--proxy-mode=ipvs \\
--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig"

EOF

cat <<EOF >/usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/kube-proxy
ExecStart=/opt/kubernetes/bin/kube-proxy \$KUBE_PROXY_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable kube-proxy
systemctl restart kube-proxy
```

执行下脚本注意参数是node节点的IP地址 `sudo bash proxy.sh 10.211.55.12`

查看下进程 `ps -ef | grep proxy`

```bash
[k8s@k8s-node01 node]$ sudo bash proxy.sh 10.211.55.12
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service.
[k8s@k8s-node01 node]$ ps -ef | grep proxy
root     13409     1  0 21:10 ?        00:00:00 /opt/kubernetes/bin/kube-proxy --logtostderr=false --log-dir=/opt/kubernetes/logs --v=4 --hostname-override=10.211.55.12 --cluster-cidr=10.0.0.0/24 --proxy-mode=ipvs --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig
k8s      13615 15286  0 21:11 pts/3    00:00:00 grep --color=auto proxy
```

这样 node 的**kubelet**和**kube-proxy**两个组件都已经部署完成了

也加入到集群中了

```bash
[k8s@k8s-master01 bin]$ kubectl get node
NAME           STATUS   ROLES    AGE   VERSION
10.211.55.12   Ready    <none>   15m   v1.13.4
```

### <a name="part6"></a>6. 扩容Node节点

我们只需要把部署的第一个**Node**文件拷贝到第二个**Node**节点上就可以了

`sudo scp -r /opt/kubernetes/ root@10.211.55.13:/opt/`

继续拷贝 service

```bash
sudo scp /usr/lib/systemd/system/{kubelet,kube-proxy}.service root@10.211.55.13:/usr/lib/systemd/system/
```

拷贝完成之后需要做的事情

1. 删除它为之前的Node 节点颁发的证书 `sudo rm /opt/kubernetes/ssl/*.*` 
1. 修改当前Node节点的IP地址 **kubelet**kubelet.config**kube-proxy **这三个文件

```bash
[k8s@k8s-node02 ~]$ sudo vi /opt/kubernetes/cfg/kubelet                                             
[k8s@k8s-node02 ~]$ sudo vi /opt/kubernetes/cfg/kubelet.config 
[k8s@k8s-node02 ~]$ sudo vi /opt/kubernetes/cfg/kube-proxy
```

我们看下 kubelet.config 文件

```shell
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 10.211.55.13
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local.
failSwapOn: false
authentication:
  anonymous:
    enabled: true
```

修改之后就可以启动了 

`sudo systemctl restart kubelet`

`sudo systemctl restart kube-proxy`

```bash
[k8s@k8s-node02 ~]$ sudo systemctl restart kubelet
[k8s@k8s-node02 ~]$ sudo systemctl restart kube-proxy
[k8s@k8s-node02 ~]$ ps -ef | grep kube
root     16774 17223  0 21:36 pts/3    00:00:00 sudo vi /opt/kubernetes/cfg/kubelet.config
root     16775 16774  0 21:36 pts/3    00:00:00 vi /opt/kubernetes/cfg/kubelet.config
root     25183     1  0 09:16 ?        00:00:43 /opt/kubernetes/bin/flanneld --ip-masq --etcd-endpoints=https://10.211.55.10:2379,https://10.211.55.12:2379,https://10.211.55.13:2379 -etcd-cafile=/opt/etcd/ssl/ca.pem -etcd-certfile=/opt/etcd/ssl/server.pem -etcd-keyfile=/opt/etcd/ssl/server-key.pem
root     26796     1  2 23:46 ?        00:00:00 /opt/kubernetes/bin/kubelet --logtostderr=false --log-dir=/opt/kubernetes/logs --v=4 --hostname-override=10.211.55.13 --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig --config=/opt/kubernetes/cfg/kubelet.config --cert-dir=/opt/kubernetes/ssl --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
root     26831     1  2 23:46 ?        00:00:00 /opt/kubernetes/bin/kube-proxy --logtostderr=false --log-dir=/opt/kubernetes/logs --v=4 --hostname-override=10.211.55.13 --cluster-cidr=10.0.0.0/24 --proxy-mode=ipvs --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig
k8s      27051 26756  0 23:46 pts/4    00:00:00 grep --color=auto kube
```

我们在mater节点，看有没有新的节点加入进来

`kubectl get csr`新的节点加入进来

`kubectl certificate approve node-csr-sMJO_rDJIrM7xhOGmMZKLTlN5XTadRuePZnrQVxg1eQ` 颁发证书

`kubectl get node` 查看node就发现新的节点加入进来

```bash
[k8s@k8s-master01 ~]$ kubectl get csr
NAME                                                   AGE    REQUESTOR           CONDITION
node-csr-sMJO_rDJIrM7xhOGmMZKLTlN5XTadRuePZnrQVxg1eQ   106s   kubelet-bootstrap   Pending
[k8s@k8s-master01 ~]$ kubectl certificate approve node-csr-sMJO_rDJIrM7xhOGmMZKLTlN5XTadRuePZnrQVxg1eQ
certificatesigningrequest.certificates.k8s.io/node-csr-sMJO_rDJIrM7xhOGmMZKLTlN5XTadRuePZnrQVxg1eQ approved
[k8s@k8s-master01 ~]$ kubectl get node
NAME           STATUS   ROLES    AGE    VERSION
10.211.55.12   Ready    <none>   170m   v1.13.4
10.211.55.13   Ready    <none>   20s    v1.13.4
```

一个**master**两个**node**就部署好了**kubernetes**集群了

![4.png](https://cdn.nlark.com/yuque/0/2019/png/288708/1553600955276-0485f1cb-f4d1-4917-9644-7c43aeb849cd.png#align=left&display=inline&height=361&name=4.png&originHeight=497&originWidth=1028&size=27106&status=done&width=746)<br />单**Master**集群架构图

### <a name="part7"></a>7. 部署一个Nginx测试示例

我们先到**master**节点操作

`kubectl create deployment nginx --image=nginx`创建一个nginx容器

`kubectl get pod`可以查看pod状态

`kubectl scale deployment nginx --replicas=3` 由一个容器扩容到3个容器

`kubectl get pods -o wide` 可以查看分布到哪个节点<br />   <br />`kubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort` 通过临时命令创建**service**暴露端口才可以对外访问

`kubectl get svc nginx` 

```bash
[k8s@k8s-master01 ~]$ kubectl get svc nginx
NAME    TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
nginx   NodePort   10.0.0.31    <none>        88:48187/TCP   4m59s
```

可以在 node 节点访问 88 内部端口

```bash
[k8s@k8s-node01 ~]$ curl 10.0.0.31:88
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

也可以访问 **node** 节点的 **ip** 加上随机生成的 48187 端口，就可以访问到部署的应用

查看日志 `kubectl logs +NAME` 需要授权

```bash
[k8s@k8s-master01 ~]$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
[k8s@k8s-master01 ~]$ kubectl get pods
NAME                   READY   STATUS              RESTARTS   AGE
nginx-5c7588df-p9m5m   0/1     ContainerCreating   0          9s
[k8s@k8s-master01 ~]$ kubectl scale deployment nginx --replicas=3
deployment.extensions/nginx scaled
[k8s@k8s-master01 ~]$ kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
nginx-5c7588df-2bmzw   1/1     Running   0          68s
nginx-5c7588df-ljzp5   1/1     Running   0          68s
nginx-5c7588df-p9m5m   1/1     Running   0          3m19s
[k8s@k8s-master01 ~]$ kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
nginx-5c7588df-2bmzw   1/1     Running   0          73s     172.17.68.3   10.211.55.12   <none>           <none>
nginx-5c7588df-ljzp5   1/1     Running   0          73s     172.17.40.2   10.211.55.13   <none>           <none>
nginx-5c7588df-p9m5m   1/1     Running   0          3m24s   172.17.68.2   10.211.55.12   <none>           <none>
[k8s@k8s-master01 ~]$ kubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort
service/nginx exposed
[k8s@k8s-master01 ~]$ kubectl get svc nginx
NAME    TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
nginx   NodePort   10.0.0.31    <none>        88:48187/TCP   4m59s
[k8s@k8s-master01 ~]$ kubectl logs nginx-5c7588df-2bmzw
Error from server (Forbidden): 
Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy) ( pods/log nginx-5c7588df-2bmzw)
```

授权：`kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous`

再查看日志就可以了

```bash
[k8s@k8s-master01 ~]$ kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous
clusterrolebinding.rbac.authorization.k8s.io/cluster-system-anonymous created
[k8s@k8s-master01 ~]$ kubectl logs nginx-5c7588df-2bmzw
10.0.0.31 - - [25/Mar/2019:04:11:57 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"
172.17.68.1 - - [25/Mar/2019:04:13:03 +0000] "GET / HTTP/1.1" 200 612 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15" "-"
```

### <a name="serial"></a>kubernetes二进制部署系列

1. [k8s部署-Etcd数据库集群部署](http://custer.me/etcd-bin-install/)
2. [k8s部署-Flannel网络](http://custer.me/flannel-bin-install/)
3. [k8s部署-Master组件](http://custer.me/kube-master/)
4. [k8s部署-Node组件](http://custer.me/kube-node/)
5. [k8s部署-多Master集群](http://custer.me/multi-master/)
